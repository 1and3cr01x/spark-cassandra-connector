package com.datastax.spark.connector.writer

import java.util

import com.datastax.driver.core._
import com.datastax.spark.connector.BatchSize
import com.datastax.spark.connector.util.PriorityHashMap
import com.google.common.collect.AbstractIterator

import scala.annotation.tailrec
import scala.collection.Iterator

/**
 * A grouping batch builder is an iterator which take an iterator of single data items and tries to group
 * those items into batches. For each data item, a batch key is computed with the provided function.
 * The items for which the batch key is the same, are grouped together into a batch.
 *
 * When the batch key for the consecutive data items is different, the items are added to separate
 * batches, and those batches are added to the queue. The queue length is limited, therefore when it is
 * full, the longest batch is removed and returned by the iterator.
 * A batch is removed from the queue also in the case when it reaches the batch size limit.
 *
 * The implementation is based on `PriorityHashMap`.
 *
 * @param batchStatementBuilder a configured batch statement builder
 * @param batchKeyGenerator     a key generator for batches - statements with the same key generated by
 *                              this function are grouped together into batches
 * @param batchSize             maximum batch size
 * @param maxBatches            maximum number of batches which can remain in the buffer
 * @param data                  data iterator
 * @tparam T                    data type
 */
class GroupingBatchBuilder[T](batchStatementBuilder: BatchStatementBuilder[T],
                              batchKeyGenerator: BoundStatement => Any,
                              batchSize: BatchSize,
                              maxBatches: Int,
                              data: Iterator[T]) extends AbstractIterator[Statement] with Iterator[Statement] {
  require(maxBatches > 0)

  private[this] val batchMap = new PriorityHashMap[Any, Batch](maxBatches)
  private[this] val emptyBatches = new util.Stack[Batch]()
  emptyBatches.ensureCapacity(maxBatches + 1)

  private[this] var lastStatement: BoundStatement = null

  // last key computation could be expensive, especially it would be a token range; therefore it is better
  // to store the value in a temporary variable for subsequent usages
  private[this] var lastKey: Any = null

  /** The contract of this method is to return `Some(Batch)` if the returned batch needs to be consumed
    * first, before the given statement can be processed, that is, the statement is not processed in this
    * case. The method returns `None` if the statement is successfully added to the batch in the queue. */
  private def processStatement(batchKey: Any, boundStatement: BoundStatement): Option[Batch] = {
    batchMap.get(batchKey) match {
      case Some(batch) =>
        updateBatchInMap(batchKey, batch, boundStatement)
      case None =>
        addBatchToMap(batchKey, boundStatement)
    }
  }

  private def updateBatchInMap(batchKey: Any, batch: Batch, newStatement: BoundStatement): Option[Batch] = {
    if (batch.add(newStatement, force = false)) {
      batchMap.put(batchKey, batch)
      None
    } else {
      batchMap.remove(batchKey)
      Some(batch)
    }
  }

  private def addBatchToMap(batchKey: Any, newStatement: BoundStatement): Option[Batch] = {
    if (batchMap.size == maxBatches) {
      Some(batchMap.dequeue())
    } else {
      val batch = newBatch()
      batch.add(newStatement, force = true)
      batchMap.put(batchKey, batch)
      None
    }
  }

  /** Tries to get unused batch from the stack; if the stack is empty, it creates a new `Batch` object */
  private def newBatch(): Batch = {
    if (emptyBatches.isEmpty)
      Batch(batchSize)
    else
      emptyBatches.pop()
  }

  /** Creates a statement from `Batch` object, clears the batch and puts it back on the stack */
  private def createStmtAndReleaseBatch(batch: Batch): Statement = {
    val stmt = batchStatementBuilder.maybeCreateBatch(batch.statements)
    batch.clear()
    emptyBatches.push(batch)
    stmt
  }

  @tailrec
  final override def computeNext(): Statement = {
    if (lastStatement == null && data.hasNext) {
      lastStatement = batchStatementBuilder.bind(data.next())
      lastKey = batchKeyGenerator(lastStatement)
    }

    if (lastStatement != null) {
      processStatement(lastKey, lastStatement) match {
        case Some(batch) =>
          createStmtAndReleaseBatch(batch)
        case None =>
          lastStatement = null
          computeNext()
      }
    } else {
      if (batchMap.nonEmpty)
        createStmtAndReleaseBatch(batchMap.dequeue())
      else
        endOfData()
    }
  }

}
